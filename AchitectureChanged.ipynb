{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting accelerate\n",
      "  Downloading accelerate-1.1.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /opt/conda/lib/python3.11/site-packages (from accelerate) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (24.1)\n",
      "Requirement already satisfied: psutil in /opt/conda/lib/python3.11/site-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/lib/python3.11/site-packages (from accelerate) (6.0.1)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (2.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.21.0 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.26.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /opt/conda/lib/python3.11/site-packages (from accelerate) (0.4.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2024.6.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.11/site-packages (from huggingface-hub>=0.21.0->accelerate) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.11/site-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.11/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.11/site-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2024.7.4)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.11/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading accelerate-1.1.0-py3-none-any.whl (333 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m333.2/333.2 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: accelerate\n",
      "Successfully installed accelerate-1.1.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import logging\n",
    "import torch\n",
    "from transformers import RobertaForQuestionAnswering, RobertaTokenizer\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): huggingface.co:443\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /nur-dev/roberta-kaz-large/resolve/main/tokenizer_config.json HTTP/11\" 200 0\n",
      "DEBUG:urllib3.connectionpool:https://huggingface.co:443 \"HEAD /nur-dev/roberta-kaz-large/resolve/main/config.json HTTP/11\" 200 0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"nur-dev/roberta-kaz-large\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"nur-dev/roberta-kaz-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class CustomQuestionAnsweringModel(RobertaForQuestionAnswering):\n",
    "    def __init__(self, masked_model):\n",
    "        super().__init__(masked_model.config)\n",
    "        self.roberta.load_state_dict(masked_model.roberta.state_dict())\n",
    "\n",
    "qa_model = CustomQuestionAnsweringModel(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "print(\"CUDA available:\", torch.cuda.is_available())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_data(questions, contexts, start_positions, end_positions):\n",
    "    encodings = tokenizer(questions, contexts, truncation=True, padding=True, return_tensors=\"pt\")\n",
    "    encodings[\"start_positions\"] = torch.tensor(start_positions)\n",
    "    encodings[\"end_positions\"] = torch.tensor(end_positions)\n",
    "    return encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>context</th>\n",
       "      <th>question</th>\n",
       "      <th>answer_start</th>\n",
       "      <th>answer_text</th>\n",
       "      <th>answer_end</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Belcalis Marlenis Alm√°nzar ( /Ààb…õlk…ôliÀêz …ëÀêlÀàm...</td>\n",
       "      <td>–ö–∞—Ä–¥–∏ –ë–∏ –º—É–∑—ã–∫–∞–Ω—ã“£ “õ–∞–π –∂–∞–Ω—Ä—ã–Ω–¥–∞ ”ô–Ω –∞–π—Ç–∞–¥—ã?</td>\n",
       "      <td>164</td>\n",
       "      <td>—Ä—ç–ø</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>–ö–∞—Ä–¥–∏ –ë–∏ –¥–∏—Å–∫–æ–≥—Ä–∞—Ñ–∏—è—Å—ã –Ω–µ–≥—ñ–∑—ñ–Ω–µ–Ω —Ç—Ä–∞–ø –ø–µ–Ω R&amp;am...</td>\n",
       "      <td>–ö–∞—Ä–¥–∏ –ë–∏ –º—É–∑—ã–∫–∞–Ω—ã“£ “õ–∞–π –∂–∞–Ω—Ä—ã–Ω–¥–∞ ”ô–Ω –∞–π—Ç–∞–¥—ã?</td>\n",
       "      <td>33</td>\n",
       "      <td>—Ç—Ä–∞–ø –ø–µ–Ω R&amp;amp;B —ç–ª–µ–º–µ–Ω—Ç—Ç–µ—Ä—ñ–Ω “õ–∞–º—Ç–∏—Ç—ã–Ω —Ö–∏–ø-—Ö–æ–ø</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>–¢—ñ–∫–±“±—Ä—ã—à—Ç—ã –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–¥—ã “Ø–ª–µ–Ω–¥—ñ–≥—ñ –±—ñ—Ä–¥–µ–π –±—ñ—Ä—ñ–Ω...</td>\n",
       "      <td>“ö—ã–∑—ã–ª –∂–æ–ª–∞“õ—Ç—ã –∞–º–µ—Ä–∏–∫–∞–Ω–¥—ã“õ –∂–∞–ª–∞—É –Ω–µ–Ω—ñ –±—ñ–ª–¥—ñ—Ä–µ–¥—ñ?</td>\n",
       "      <td>537</td>\n",
       "      <td>—à—ã–¥–∞–º–¥—ã–ª—ã“õ –ø–µ–Ω –±–∞—Ç—ã—Ä–ª—ã“õ—Ç—ã</td>\n",
       "      <td>562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>–ú–æÃÅ–Ω–∞ –õ–∏ÃÅ–∑–∞ (Mona Lisa) ‚Äî –±“±–ª —à–∞–º–∞–º–µ–Ω 1503 –∂—ã–ª...</td>\n",
       "      <td>”ò–π–≥—ñ–ª—ñ '–ú–æ–Ω–∞ –õ–∏–∑–∞' –∫–∞—Ä—Ç–∏–Ω–∞—Å—ã “õ–∞–π–¥–∞ “õ–æ–π—ã–ª“ì–∞–Ω?</td>\n",
       "      <td>217</td>\n",
       "      <td>–§—Ä–∞–Ω—Ü–∏—è, –ü–∞—Ä–∏–∂)</td>\n",
       "      <td>232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>–õ–∏–≤–µ—Ä–ø—É–ª—å —Ñ—É—Ç–±–æ–ª –∫–ª—É–±—ã (Liverpool Football Clu...</td>\n",
       "      <td>–õ–∏–≤–µ—Ä–ø—É–ª—å –ü—Ä–µ–º—å–µ—Ä-–õ–∏–≥–∞ —Ç–∏—Ç—É–ª—ã–Ω “õ–∞–Ω—à–∞ —Ä–µ—Ç –∂–µ“£—ñ–ø...</td>\n",
       "      <td>208</td>\n",
       "      <td>19 —Ä–µ—Ç</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             context  \\\n",
       "0  Belcalis Marlenis Alm√°nzar ( /Ààb…õlk…ôliÀêz …ëÀêlÀàm...   \n",
       "1  –ö–∞—Ä–¥–∏ –ë–∏ –¥–∏—Å–∫–æ–≥—Ä–∞—Ñ–∏—è—Å—ã –Ω–µ–≥—ñ–∑—ñ–Ω–µ–Ω —Ç—Ä–∞–ø –ø–µ–Ω R&am...   \n",
       "2  –¢—ñ–∫–±“±—Ä—ã—à—Ç—ã –≥–æ—Ä–∏–∑–æ–Ω—Ç–∞–ª—å–¥—ã “Ø–ª–µ–Ω–¥—ñ–≥—ñ –±—ñ—Ä–¥–µ–π –±—ñ—Ä—ñ–Ω...   \n",
       "3  –ú–æÃÅ–Ω–∞ –õ–∏ÃÅ–∑–∞ (Mona Lisa) ‚Äî –±“±–ª —à–∞–º–∞–º–µ–Ω 1503 –∂—ã–ª...   \n",
       "4  –õ–∏–≤–µ—Ä–ø—É–ª—å —Ñ—É—Ç–±–æ–ª –∫–ª—É–±—ã (Liverpool Football Clu...   \n",
       "\n",
       "                                            question  answer_start  \\\n",
       "0         –ö–∞—Ä–¥–∏ –ë–∏ –º—É–∑—ã–∫–∞–Ω—ã“£ “õ–∞–π –∂–∞–Ω—Ä—ã–Ω–¥–∞ ”ô–Ω –∞–π—Ç–∞–¥—ã?           164   \n",
       "1         –ö–∞—Ä–¥–∏ –ë–∏ –º—É–∑—ã–∫–∞–Ω—ã“£ “õ–∞–π –∂–∞–Ω—Ä—ã–Ω–¥–∞ ”ô–Ω –∞–π—Ç–∞–¥—ã?            33   \n",
       "2    “ö—ã–∑—ã–ª –∂–æ–ª–∞“õ—Ç—ã –∞–º–µ—Ä–∏–∫–∞–Ω–¥—ã“õ –∂–∞–ª–∞—É –Ω–µ–Ω—ñ –±—ñ–ª–¥—ñ—Ä–µ–¥—ñ?           537   \n",
       "3       ”ò–π–≥—ñ–ª—ñ '–ú–æ–Ω–∞ –õ–∏–∑–∞' –∫–∞—Ä—Ç–∏–Ω–∞—Å—ã “õ–∞–π–¥–∞ “õ–æ–π—ã–ª“ì–∞–Ω?           217   \n",
       "4  –õ–∏–≤–µ—Ä–ø—É–ª—å –ü—Ä–µ–º—å–µ—Ä-–õ–∏–≥–∞ —Ç–∏—Ç—É–ª—ã–Ω “õ–∞–Ω—à–∞ —Ä–µ—Ç –∂–µ“£—ñ–ø...           208   \n",
       "\n",
       "                                      answer_text  answer_end  \n",
       "0                                             —Ä—ç–ø         167  \n",
       "1  —Ç—Ä–∞–ø –ø–µ–Ω R&amp;B —ç–ª–µ–º–µ–Ω—Ç—Ç–µ—Ä—ñ–Ω “õ–∞–º—Ç–∏—Ç—ã–Ω —Ö–∏–ø-—Ö–æ–ø          79  \n",
       "2                       —à—ã–¥–∞–º–¥—ã–ª—ã“õ –ø–µ–Ω –±–∞—Ç—ã—Ä–ª—ã“õ—Ç—ã         562  \n",
       "3                                 –§—Ä–∞–Ω—Ü–∏—è, –ü–∞—Ä–∏–∂)         232  \n",
       "4                                          19 —Ä–µ—Ç         214  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('final.csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encodings = encode_data(\n",
    "    data['question'].tolist(),\n",
    "    data['context'].tolist(),\n",
    "    data['answer_start'].tolist(),\n",
    "    data['answer_end'].tolist()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_dataset = QADataset(train_encodings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2080' max='2080' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2080/2080 56:03, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>5.763300</td>\n",
       "      <td>5.611176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.387200</td>\n",
       "      <td>5.053584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.968100</td>\n",
       "      <td>4.270538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.232000</td>\n",
       "      <td>3.340251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.719400</td>\n",
       "      <td>2.775655</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.119500</td>\n",
       "      <td>2.546930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>2.891000</td>\n",
       "      <td>2.473369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2.970800</td>\n",
       "      <td>2.427505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>2.543600</td>\n",
       "      <td>2.403644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.390900</td>\n",
       "      <td>2.386472</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2080, training_loss=3.762348262163309, metrics={'train_runtime': 3364.8562, 'train_samples_per_second': 19.733, 'train_steps_per_second': 0.618, 'total_flos': 6.16661442772992e+16, 'train_loss': 3.762348262163309, 'epoch': 10.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,  # –£–º–µ–Ω—å—à–∏—Ç–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    num_train_epochs=10,  # –£–≤–µ–ª–∏—á—å—Ç–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=train_dataset\n",
    ")\n",
    "\n",
    "# –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: –û—Ä—Ç–∞–ª—ã“õ –ê–∑–∏—è–¥–∞“ì—ã –µ“£ —ñ—Ä—ñ –º–µ–º–ª–µ–∫–µ—Ç—Ç–µ—Ä–¥—ñ“£ –±—ñ—Ä—ñ?\n",
      "Answer:  –µ“£ —ñ—Ä—ñ –º–µ–º–ª–µ–∫–µ—Ç—Ç–µ—Ä–¥—ñ“£ –±—ñ—Ä—ñ?</s></s>\n",
      "“ö–∞–∑–∞“õ—Å—Ç–∞–Ω –û—Ä—Ç–∞–ª—ã“õ –ê–∑–∏—è–¥–∞“ì—ã –µ“£ —ñ—Ä—ñ –º–µ–º–ª–µ–∫–µ—Ç—Ç–µ—Ä–¥—ñ“£ –±—ñ—Ä—ñ –±–æ–ª—ã–ø —Ç–∞–±—ã–ª–∞–¥—ã\n"
     ]
    }
   ],
   "source": [
    "context = \"\"\"\n",
    "“ö–∞–∑–∞“õ—Å—Ç–∞–Ω –û—Ä—Ç–∞–ª—ã“õ –ê–∑–∏—è–¥–∞“ì—ã –µ“£ —ñ—Ä—ñ –º–µ–º–ª–µ–∫–µ—Ç—Ç–µ—Ä–¥—ñ“£ –±—ñ—Ä—ñ –±–æ–ª—ã–ø —Ç–∞–±—ã–ª–∞–¥—ã.\n",
    "\"\"\"\n",
    "question = \"–û—Ä—Ç–∞–ª—ã“õ –ê–∑–∏—è–¥–∞“ì—ã –µ“£ —ñ—Ä—ñ –º–µ–º–ª–µ–∫–µ—Ç—Ç–µ—Ä–¥—ñ“£ –±—ñ—Ä—ñ?\"\n",
    "qa_model.to(\"cpu\")\n",
    "# Tokenize the input\n",
    "inputs = tokenizer.encode_plus(\n",
    "    question, \n",
    "    context, \n",
    "    add_special_tokens=True, \n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "input_ids = inputs[\"input_ids\"]\n",
    "attention_mask = inputs[\"attention_mask\"]\n",
    "\n",
    "# Perform inference\n",
    "with torch.no_grad():\n",
    "    outputs = qa_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "    start_logits = outputs.start_logits\n",
    "    end_logits = outputs.end_logits\n",
    "\n",
    "# Find the answer's start and end position\n",
    "start_index = torch.argmax(start_logits)\n",
    "end_index = torch.argmax(end_logits)\n",
    "\n",
    "# Decode the answer from the context\n",
    "answer = tokenizer.decode(input_ids[0][start_index:end_index + 1])\n",
    "\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CustomQuestionAnsweringModel(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=1024, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Nov  3 07:42:37 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.183.01             Driver Version: 535.183.01   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA L40S                    On  | 00000000:04:00.0 Off |                    0 |\n",
      "| N/A   64C    P0              99W / 350W |      0MiB / 46068MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18M\t./results/checkpoint-3000\n"
     ]
    }
   ],
   "source": [
    "!du -sh ./results/checkpoint-4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
